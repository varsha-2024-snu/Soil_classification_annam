{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":102672,"databundleVersionId":12375409,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a6d9af7a-4458-4cb6-817e-c75534bd865e","cell_type":"markdown","source":"# Soil Type Classification using ResNet34 -ANNAM.AI\nThis project addresses a soil classification task as part of a Kaggle competition hosted during ANNAM.AI orientation. A custom PyTorch pipeline is used to classify soil images into four categories using a fine-tuned ResNet34 model.\n","metadata":{}},{"id":"1b1a25fd-eab4-4ab8-b786-685a4d017949","cell_type":"markdown","source":"## Importing Libraries\n\nWe start by importing all the necessary libraries used throughout the project:\n\n- **Core libraries:** For file handling and data manipulation (`os`, `shutil`, `pandas`, `numpy`).\n- **PyTorch and torchvision:** For building and training deep learning models, working with datasets, and image transformations.\n- **PIL:** For image processing.\n- **Scikit-learn:** For dataset splitting and evaluation metrics (F1 score).\n","metadata":{}},{"id":"369a99cc","cell_type":"code","source":"# Core libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport shutil\n\n# PyTorch and vision tools\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import resnet34, ResNet34_Weights\nfrom PIL import Image\n\n# Sklearn tools for evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:20:03.264281Z","iopub.execute_input":"2025-05-23T11:20:03.264507Z","iopub.status.idle":"2025-05-23T11:20:14.316192Z","shell.execute_reply.started":"2025-05-23T11:20:03.264482Z","shell.execute_reply":"2025-05-23T11:20:14.315454Z"}},"outputs":[],"execution_count":1},{"id":"3187cda1-bd7c-4ab8-b399-625a6a321a89","cell_type":"markdown","source":"## Loading and Preparing the Training Data\n\n- Loaded the CSV file containing the training labels.\n- Created a label mapping to convert soil type names into numeric labels for modeling.\n- Defined the directory path containing training images.\n- Since image filenames may have different extensions (`.jpg`, `.jpeg`, `.png`), implemented a function to correctly resolve the actual filename for each image.\n- Applied this function to map each image ID to its resolved filename.\n- Removed any rows where the image file could not be found to ensure data consistency.\n","metadata":{}},{"id":"a6e92145-0cdf-49f7-b543-20d9654caa5d","cell_type":"code","source":"# Load the training labels CSV\ntrain_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv')\n\n# Map soil type strings to numeric labels\nlabel_map = {\n    'Alluvial soil': 0,\n    'Black Soil': 1,\n    'Clay soil': 2,\n    'Red soil': 3\n}\ntrain_df['label'] = train_df['soil_type'].map(label_map)\n\n# Define the training image directory\ntrain_dir = '/kaggle/input/soil-classification/soil_classification-2025/train'\n\n# Resolve actual image filenames (handles .jpg/.jpeg/.png extensions)\ntrain_files = os.listdir(train_dir)\ntrain_files_lower = {f.lower(): f for f in train_files}\n\ndef resolve_image_file(image_id):\n    base = os.path.splitext(image_id)[0]\n    for ext in ['.jpg', '.jpeg', '.png']:\n        fname = f\"{base}{ext}\"\n        if fname.lower() in train_files_lower:\n            return train_files_lower[fname.lower()]\n    return None\n\n# Apply filename resolution\ntrain_df['resolved_image'] = train_df['image_id'].apply(resolve_image_file)\ntrain_df = train_df.dropna(subset=['resolved_image']).reset_index(drop=True)\ntrain_df['image_id'] = train_df['resolved_image']\ntrain_df.drop(columns=['resolved_image'], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:23:37.578403Z","iopub.execute_input":"2025-05-23T11:23:37.578695Z","iopub.status.idle":"2025-05-23T11:23:37.635319Z","shell.execute_reply.started":"2025-05-23T11:23:37.578673Z","shell.execute_reply":"2025-05-23T11:23:37.634588Z"}},"outputs":[],"execution_count":2},{"id":"04343463-ad38-4237-b52a-636082866d00","cell_type":"markdown","source":"## Data Transformation and Custom Dataset\n\n- Set the target image size to 224x224 pixels.\n- Defined normalization parameters matching pretrained model expectations.\n- Created data augmentation pipeline for training, including resizing, random horizontal flips, rotations, and color jitter to improve model robustness.\n- Defined a simpler transformation for validation and testing without augmentation.\n- Implemented a custom `SoilDataset` class to:\n  - Load images from disk.\n  - Apply transformations.\n  - Return image-label pairs for model training.\n","metadata":{}},{"id":"1d115466-59f5-47d8-815d-f627057f1932","cell_type":"code","source":"# Image size and normalization\nIMG_SIZE = 224\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Augmentation for training\ntransform_train = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Minimal transform for validation/test\ntransform_val = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Custom dataset class for loading soil images\nclass SoilDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.img_dir, row.image_id)\n        image = Image.open(image_path).convert(\"RGB\")\n        label = row.label\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:23:51.504155Z","iopub.execute_input":"2025-05-23T11:23:51.504928Z","iopub.status.idle":"2025-05-23T11:23:51.512257Z","shell.execute_reply.started":"2025-05-23T11:23:51.504892Z","shell.execute_reply":"2025-05-23T11:23:51.511534Z"}},"outputs":[],"execution_count":3},{"id":"44304830-0397-4849-8bbe-93aa5d88077d","cell_type":"markdown","source":"## Train-Validation Split and Model Setup\n\n- Split the data into training and validation sets with stratification to maintain label distribution.\n- Created custom dataset objects and corresponding dataloaders with batch size 32.\n- Loaded a pretrained ResNet34 model and replaced its final fully connected layer to output predictions for 4 soil classes.\n- Set up the device to use GPU if available for faster training.\n- Defined the loss function as Cross-Entropy Loss suitable for multi-class classification.\n- Used the Adam optimizer with a learning rate of 0.0001.\n- Added a learning rate scheduler to reduce the learning rate by half every 5 epochs to improve convergence.\n","metadata":{}},{"id":"adbfc26b-89cd-4b3b-8fb2-b207d7762f43","cell_type":"code","source":"# Split data into train and validation\ntrain_df, val_df = train_test_split(train_df, stratify=train_df['label'], test_size=0.2, random_state=42)\n\n# Create dataset and dataloaders\ntrain_ds = SoilDataset(train_df, train_dir, transform=transform_train)\nval_ds = SoilDataset(val_df, train_dir, transform=transform_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n\n# Load ResNet34 with pretrained weights\nweights = ResNet34_Weights.DEFAULT\nmodel = resnet34(weights=weights)\n\n# Replace final classification layer to match 4 soil classes\nmodel.fc = nn.Linear(model.fc.in_features, 4)\n\n# Move model to GPU/CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss, optimizer, and learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:23:56.563486Z","iopub.execute_input":"2025-05-23T11:23:56.563737Z","iopub.status.idle":"2025-05-23T11:23:57.808010Z","shell.execute_reply.started":"2025-05-23T11:23:56.563711Z","shell.execute_reply":"2025-05-23T11:23:57.807232Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 210MB/s]\n","output_type":"stream"}],"execution_count":4},{"id":"18f29b18-bdfa-4d61-9deb-d30ac6695e93","cell_type":"markdown","source":"## Model Training and Evaluation\n\n- Defined a `train_model` function that trains the model for a specified number of epochs.\n- In each epoch:\n  - Set the model to training mode and computed the training loss.\n  - Performed backpropagation and optimizer steps.\n  - Updated the learning rate scheduler.\n- After each epoch, switched to evaluation mode:\n  - Predicted on the validation set.\n  - Calculated per-class F1 scores to assess performance on each soil type.\n  - Printed training loss and validation F1 scores for monitoring.\n- Trained the model for 15 epochs.\n- Saved the final trained model weights to `best_model.pth` for later use.\n","metadata":{}},{"id":"ae7843a7-8468-49c5-a76e-6ad1beb65de7","cell_type":"code","source":"def train_model(model, epochs):\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        scheduler.step()\n\n        # Validation phase\n        model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        f1 = f1_score(all_labels, all_preds, average=None)\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"  ➤ Train Loss: {running_loss / len(train_loader):.4f}\")\n        print(f\"  ➤ Per-class F1: {f1}\")\n        print(f\"  ➤ Min F1: {min(f1):.4f}\")\n\n# Train the model\ntrain_model(model, epochs=15)\n\n# Save the best trained model\ntorch.save(model.state_dict(), \"best_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:24:07.075971Z","iopub.execute_input":"2025-05-23T11:24:07.076451Z","iopub.status.idle":"2025-05-23T11:28:28.639122Z","shell.execute_reply.started":"2025-05-23T11:24:07.076428Z","shell.execute_reply":"2025-05-23T11:28:28.638485Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/15\n  ➤ Train Loss: 0.4024\n  ➤ Per-class F1: [0.95327103 0.91489362 0.94736842 0.98039216]\n  ➤ Min F1: 0.9149\n\nEpoch 2/15\n  ➤ Train Loss: 0.1459\n  ➤ Per-class F1: [0.96650718 0.95652174 0.93975904 0.98039216]\n  ➤ Min F1: 0.9398\n\nEpoch 3/15\n  ➤ Train Loss: 0.1204\n  ➤ Per-class F1: [0.94339623 0.9375     0.91358025 0.92783505]\n  ➤ Min F1: 0.9136\n\nEpoch 4/15\n  ➤ Train Loss: 0.0828\n  ➤ Per-class F1: [0.94       0.96774194 0.87640449 0.98076923]\n  ➤ Min F1: 0.8764\n\nEpoch 5/15\n  ➤ Train Loss: 0.0607\n  ➤ Per-class F1: [0.96039604 0.94845361 0.94117647 0.98039216]\n  ➤ Min F1: 0.9412\n\nEpoch 6/15\n  ➤ Train Loss: 0.0469\n  ➤ Per-class F1: [0.98076923 0.9787234  0.96296296 0.99029126]\n  ➤ Min F1: 0.9630\n\nEpoch 7/15\n  ➤ Train Loss: 0.0318\n  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n  ➤ Min F1: 0.9756\n\nEpoch 8/15\n  ➤ Train Loss: 0.0282\n  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n  ➤ Min F1: 0.9756\n\nEpoch 9/15\n  ➤ Train Loss: 0.0353\n  ➤ Per-class F1: [0.97584541 0.9787234  0.96296296 1.        ]\n  ➤ Min F1: 0.9630\n\nEpoch 10/15\n  ➤ Train Loss: 0.0532\n  ➤ Per-class F1: [0.98076923 0.97826087 0.97560976 1.        ]\n  ➤ Min F1: 0.9756\n\nEpoch 11/15\n  ➤ Train Loss: 0.0234\n  ➤ Per-class F1: [0.98058252 0.9787234  0.97560976 1.        ]\n  ➤ Min F1: 0.9756\n\nEpoch 12/15\n  ➤ Train Loss: 0.0202\n  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n  ➤ Min F1: 0.9756\n\nEpoch 13/15\n  ➤ Train Loss: 0.0154\n  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n  ➤ Min F1: 0.9756\n\nEpoch 14/15\n  ➤ Train Loss: 0.0201\n  ➤ Per-class F1: [0.98058252 0.96842105 0.97560976 0.99029126]\n  ➤ Min F1: 0.9684\n\nEpoch 15/15\n  ➤ Train Loss: 0.0250\n  ➤ Per-class F1: [0.97142857 0.9787234  0.93670886 0.99029126]\n  ➤ Min F1: 0.9367\n","output_type":"stream"}],"execution_count":5},{"id":"d69a7756-9d47-4432-97f0-77fb399e5d7a","cell_type":"markdown","source":"## Preparing Test Data\n\n- Copied test images from the input directory to the working directory to allow image format conversions.\n- Loaded the CSV file containing test image IDs.\n- Converted unsupported image formats (`.webp`, `.gif`) to `.jpg` for compatibility with the model.\n- Implemented a function to resolve the correct image filename considering possible extensions (`.jpg`, `.jpeg`, `.png`).\n- Applied this function to ensure all test image IDs map to valid image files, dropping any missing files to maintain data integrity.\n","metadata":{}},{"id":"10331740-e732-4d1d-8379-1959fa14fd5b","cell_type":"code","source":"# Copy test files to working directory to allow .jpg conversion\ntest_dir = '/kaggle/input/soil-classification/soil_classification-2025/test'\ntest_ids_path = '/kaggle/input/soil-classification/soil_classification-2025/test_ids.csv'\nworking_test_dir = '/kaggle/working/test'\n\nif not os.path.exists(working_test_dir):\n    shutil.copytree(test_dir, working_test_dir)\n\n# Load test image IDs\ntest_df = pd.read_csv(test_ids_path)\n\n# Handle .webp/.gif files → .jpg\ntest_files = os.listdir(working_test_dir)\ntest_files_lower = {f.lower(): f for f in test_files}\n\nfor fname in test_files:\n    ext = os.path.splitext(fname)[1].lower()\n    if ext in ['.webp', '.gif']:\n        base = os.path.splitext(fname)[0]\n        new_path = os.path.join(working_test_dir, base + \".jpg\")\n        if not os.path.exists(new_path):\n            try:\n                img = Image.open(os.path.join(working_test_dir, fname)).convert(\"RGB\")\n                img.save(new_path, format=\"JPEG\")\n                print(f\"Converted {fname} → {base}.jpg\")\n            except Exception as e:\n                print(f\"Failed to convert {fname}: {e}\")\n\n# Resolve file paths in test folder\ndef resolve_file(image_id):\n    base = os.path.splitext(image_id)[0]\n    for ext in ['.jpg', '.jpeg', '.png']:\n        fname = f\"{base}{ext}\"\n        if fname.lower() in test_files_lower:\n            return test_files_lower[fname.lower()]\n        full_path = os.path.join(working_test_dir, fname)\n        if os.path.exists(full_path):\n            return fname\n    return None\n\n# Apply resolution\ntest_df['resolved_image'] = test_df['image_id'].apply(resolve_file)\ntest_df = test_df.dropna(subset=['resolved_image']).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:28:46.985864Z","iopub.execute_input":"2025-05-23T11:28:46.986645Z","iopub.status.idle":"2025-05-23T11:28:48.902913Z","shell.execute_reply.started":"2025-05-23T11:28:46.986615Z","shell.execute_reply":"2025-05-23T11:28:48.902124Z"}},"outputs":[{"name":"stdout","text":"Converted img_91cbc6e5.gif → img_91cbc6e5.jpg\nConverted img_f22972ea.webp → img_f22972ea.jpg\n","output_type":"stream"}],"execution_count":6},{"id":"9bc9cefc-a706-441d-83b7-52874dfeab66","cell_type":"markdown","source":"## Test Dataset and DataLoader for Inference\n\n- Defined the test transformation pipeline, which is the same as the validation transforms (resize + normalization).\n- Created a custom `TestDataset` class to:\n  - Load and transform test images.\n  - Return the transformed image along with its original image ID.\n- Initialized the test dataset and corresponding dataloader for batch inference with a batch size of 32.\n","metadata":{}},{"id":"ee21f7aa-a666-460e-9307-e68c5ec4c149","cell_type":"code","source":"# Final test transform (same as val)\ntest_transform = transform_val\n\n# Create dataset for test inference\nclass TestDataset(Dataset):\n    def __init__(self, df, img_dir, transform):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = os.path.join(self.img_dir, row['resolved_image'])\n        image = Image.open(path).convert(\"RGB\")\n        image = self.transform(image)\n        return image, row['image_id']\n\ntest_dataset = TestDataset(test_df, working_test_dir, test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:30:52.162457Z","iopub.execute_input":"2025-05-23T11:30:52.162751Z","iopub.status.idle":"2025-05-23T11:30:52.168757Z","shell.execute_reply.started":"2025-05-23T11:30:52.162729Z","shell.execute_reply":"2025-05-23T11:30:52.167991Z"}},"outputs":[],"execution_count":7},{"id":"dee2cf6d-5dac-4fe2-afa2-a9bfff7a9c95","cell_type":"markdown","source":"## Model Inference on Test Data\n\n- Reloaded the pretrained ResNet34 model architecture and replaced the final layer to match the 4 soil classes.\n- Loaded the saved trained weights (`best_model.pth`) into the model.\n- Set the model to evaluation mode and moved it to the appropriate device (GPU/CPU).\n- Created a reverse mapping from numeric labels back to soil type names.\n- Ran inference on the test dataset:\n  - Predicted the soil class for each test image.\n  - Stored the predictions along with their corresponding image IDs for submission.\n","metadata":{}},{"id":"48fc1045-c16b-499b-a080-402f590d7e0f","cell_type":"code","source":"# Load trained model weights\nmodel = resnet34(weights=ResNet34_Weights.DEFAULT)\nmodel.fc = nn.Linear(model.fc.in_features, 4)\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel = model.to(device)\nmodel.eval()\n\n# Label decoding map\ninv_label_map = {0: 'Alluvial soil', 1: 'Black Soil', 2: 'Clay soil', 3: 'Red soil'}\n\n# Predict test labels\npredictions = []\nwith torch.no_grad():\n    for images, image_ids in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        for image_id, pred in zip(image_ids, preds):\n            predictions.append((image_id, inv_label_map[pred]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:30:57.133820Z","iopub.execute_input":"2025-05-23T11:30:57.134076Z","iopub.status.idle":"2025-05-23T11:30:59.997677Z","shell.execute_reply.started":"2025-05-23T11:30:57.134056Z","shell.execute_reply":"2025-05-23T11:30:59.996888Z"}},"outputs":[],"execution_count":8},{"id":"88a4f03f-cf1d-46e7-a089-8262781c0d67","cell_type":"markdown","source":"## Creating Submission File\n\n- Created a DataFrame from the list of predictions containing image IDs and their predicted soil types.\n- Saved the DataFrame to a CSV file named `submission.csv` without the index column.\n- Printed confirmation showing the number of rows generated in the submission file.\n","metadata":{}},{"id":"84b18603-3675-4503-918a-3d2d1fc8b93b","cell_type":"code","source":"# Create submission DataFrame and save to CSV\nsubmission = pd.DataFrame(predictions, columns=[\"image_id\", \"soil_type\"])\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"submission.csv generated with\", len(submission), \"rows.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:31:07.878509Z","iopub.execute_input":"2025-05-23T11:31:07.878873Z","iopub.status.idle":"2025-05-23T11:31:07.889343Z","shell.execute_reply.started":"2025-05-23T11:31:07.878843Z","shell.execute_reply":"2025-05-23T11:31:07.888649Z"}},"outputs":[{"name":"stdout","text":"✅ submission.csv generated with 341 rows.\n","output_type":"stream"}],"execution_count":9}]}